{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torch.optim import SGD"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset\n",
    "- Before we load the dataset, we need to define a transformation.\n",
    "- A transformation is applied to the data before feeding it to the Neural Network.\n",
    "- These transformations mostly consist of techniques used for data augmentation.\n",
    "- In this example, we will not be using data augmentation.\n",
    "- We will make use of transformation just to convert images to their tensor representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining transformation to convert images to tensor\n",
    "transform = torchvision.transforms.Compose([\n",
    "            torchvision.transforms.ToTensor()\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load our training data from the torchvision datasets module.\n",
    "train_data = torchvision.datasets.MNIST(root = './19_train', train = True, download = True, transform = transform)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = torchvision.datasets.MNIST(root = './19_test', train = False, download = True, transform = transform)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "take out half of the test data as validation data for performing validation while training the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_data, test_data = random_split(test_data, [5000, 5000])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Data Loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_data, batch_size = 32)\n",
    "valid_dataloader = DataLoader(valid_data, batch_size = 32)\n",
    "test_dataloader = DataLoader(test_data, batch_size = 32)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()\n",
    "        self.layer1=nn.Linear(784, 300)\n",
    "        self.layer2=nn.Linear(300, 100)\n",
    "        self.layer3=nn.Linear(100, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x=x.view(-1, 784)\n",
    "        x=F.relu(self.layer1(x))\n",
    "        x=F.relu(self.layer2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MyNet(\n",
      "  (layer1): Linear(in_features=784, out_features=300, bias=True)\n",
      "  (layer2): Linear(in_features=300, out_features=100, bias=True)\n",
      "  (layer3): Linear(in_features=100, out_features=10, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create out Neural Network by creating an instance of the MyNet class\n",
    "model = MyNet()\n",
    "print(model)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Optimizer and Loss Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    differentiable: False\n",
      "    foreach: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    momentum: 0\n",
      "    nesterov: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# create a Stochastic Gradient Descent(SGD) optimizer to optimize the parameters of our model and set its learning rate to 0.001.\n",
    "optimizer = SGD(model.parameters(), lr = 0.001)\n",
    "print(optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# create a loss function to see how good or bad our model is performing at a given time.\n",
    "loss_function = torch.nn.CrossEntropyLoss()\n",
    "print(loss_function)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performing Training and Validation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train the model on the training dataset, while simultaneously measuring how the model performs on the the validation dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "Training loss: 3.553650709025065\n",
      "Validation loss: 2.4443936568156928\n",
      "Epoch: 1\n",
      "Training loss: 2.080358124860128\n",
      "Validation loss: 1.4499497064359628\n",
      "Epoch: 2\n",
      "Training loss: 1.1282290950934093\n",
      "Validation loss: 0.8742818574237216\n",
      "Epoch: 3\n",
      "Training loss: 0.7909886005083719\n",
      "Validation loss: 0.6718631428518113\n",
      "Epoch: 4\n",
      "Training loss: 0.6476645072698594\n",
      "Validation loss: 0.5691200978816695\n",
      "Epoch: 5\n",
      "Training loss: 0.5677690280357996\n",
      "Validation loss: 0.506357674575915\n",
      "Epoch: 6\n",
      "Training loss: 0.5162254646341006\n",
      "Validation loss: 0.46394608535204723\n",
      "Epoch: 7\n",
      "Training loss: 0.48007792270978294\n",
      "Validation loss: 0.43342285418206716\n",
      "Epoch: 8\n",
      "Training loss: 0.453288878150781\n",
      "Validation loss: 0.4104569557176274\n",
      "Epoch: 9\n",
      "Training loss: 0.4326092449426651\n",
      "Validation loss: 0.39255966265110454\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(10):\n",
    "\n",
    "    # Performing Training for each epoch\n",
    "    training_loss = 0.\n",
    "    model.train()\n",
    "\n",
    "    # The training loop\n",
    "    for batch in train_dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        input, label = batch\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, label)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        training_loss += loss.item()\n",
    "\n",
    "\n",
    "    # Performing Validation for each epoch\n",
    "    validation_loss = 0.\n",
    "    model.eval()\n",
    "\n",
    "    # The validation loop\n",
    "    for batch in valid_dataloader:\n",
    "        input, label = batch\n",
    "        output = model(input)\n",
    "        loss = loss_function(output, label)\n",
    "        validation_loss += loss.item()\n",
    "\n",
    "    # Calculating the average training and validation loss over epoch\n",
    "    training_loss_avg = training_loss/len(train_dataloader)\n",
    "    validation_loss_avg = validation_loss/len(valid_dataloader)\n",
    "\n",
    "    # Printing average training and average validation losses\n",
    "    print(\"Epoch: {}\".format(epoch))\n",
    "    print(\"Training loss: {}\".format(training_loss_avg))\n",
    "    print(\"Validation loss: {}\".format(validation_loss_avg))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8877388535031847\n"
     ]
    }
   ],
   "source": [
    "# Setting the number of correct predictions to 0\n",
    "num_correct_pred = 0\n",
    "\n",
    "# Running the model over test dataset and calculating total correct predictions\n",
    "for batch in test_dataloader:\n",
    "        input, label = batch\n",
    "        output = model(input)\n",
    "        _, predictions = torch.max(output.data, 1)\n",
    "        num_correct_pred += (predictions == label).sum().item()\n",
    "\n",
    "# Calculating the accuracy of model on test dataset\n",
    "accuracy = num_correct_pred/(len(test_dataloader)*test_dataloader.batch_size)\n",
    "\n",
    "print(accuracy)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[6.8378e+00, 5.1838e+00, 5.7442e+00, 1.0463e+01, 6.0611e+00, 1.1388e+01,\n",
      "         6.5693e+00, 5.1592e+00, 1.0250e+01, 7.4876e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.7052e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.0965e-01,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 1.1314e-01, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 4.4714e-02, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 3.5209e-02,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 6.1360e-02,\n",
      "         0.0000e+00, 2.4796e-01, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 6.9357e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 4.4562e-02, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 2.9068e-03, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00, 0.0000e+00,\n",
      "         1.7522e-01, 0.0000e+00, 6.1931e-03, 0.0000e+00]],\n",
      "       grad_fn=<ReluBackward0>)\n"
     ]
    }
   ],
   "source": [
    "# Making Prediction\n",
    "prediction = model(test_data[1][0])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "print(prediction.argmax().item())\n",
    "# Outputs- 5"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch-basic",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
